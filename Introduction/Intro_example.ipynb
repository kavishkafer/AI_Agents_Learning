{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-05T05:49:36.387988Z",
     "start_time": "2025-06-05T05:49:31.862205Z"
    }
   },
   "source": [
    "!pip install -q huggingface_hub\n",
    "!pip install litellm"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litellm in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (1.72.1)\n",
      "Requirement already satisfied: aiohttp in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from litellm) (3.12.9)\n",
      "Requirement already satisfied: click in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from litellm) (8.2.1)\n",
      "Requirement already satisfied: httpx>=0.23.0 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from litellm) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from litellm) (8.7.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from litellm) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from litellm) (4.24.0)\n",
      "Requirement already satisfied: openai>=1.68.2 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from litellm) (1.84.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from litellm) (2.11.5)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from litellm) (1.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from litellm) (0.9.0)\n",
      "Requirement already satisfied: tokenizers in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from litellm) (0.21.1)\n",
      "Requirement already satisfied: anyio in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from httpx>=0.23.0->litellm) (4.9.0)\n",
      "Requirement already satisfied: certifi in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from httpx>=0.23.0->litellm) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from httpx>=0.23.0->litellm) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from httpx>=0.23.0->litellm) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from importlib-metadata>=6.8.0->litellm) (3.22.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.25.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from openai>=1.68.2->litellm) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from openai>=1.68.2->litellm) (0.10.0)\n",
      "Requirement already satisfied: sniffio in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from openai>=1.68.2->litellm) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from openai>=1.68.2->litellm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from openai>=1.68.2->litellm) (4.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->litellm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->litellm) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->litellm) (0.4.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from tiktoken>=0.7.0->litellm) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from tiktoken>=0.7.0->litellm) (2.32.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from aiohttp->litellm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from aiohttp->litellm) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from aiohttp->litellm) (1.6.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from aiohttp->litellm) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from aiohttp->litellm) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from aiohttp->litellm) (1.20.0)\n",
      "Requirement already satisfied: colorama in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from click->litellm) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from tokenizers->litellm) (0.32.4)\n",
      "Requirement already satisfied: filelock in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2025.5.1)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\4thyear\\huggingface_course\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T05:48:46.121111Z",
     "start_time": "2025-06-05T05:48:46.114696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "HF_Token = os.getenv(\"HF_TOKEN\")\n"
   ],
   "id": "991211717db2f2b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T05:55:30.358075Z",
     "start_time": "2025-06-05T05:55:30.256671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from smolagents import LiteLLMModel\n",
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "model = LiteLLMModel(\n",
    "    model_id=\"ollama_chat/qwen2.5:latest\",\n",
    "    api_base=\"http://localhost:11434\",\n",
    "    num_ctx=8192,\n",
    ")\n",
    "client = InferenceClient(provider=\"hf-inference\", model=\"meta-llama/Llama-3.3-70B-Instruct\");"
   ],
   "id": "1dae7e7ccb255005",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "835f0b83dfa0fda2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T05:52:19.759034Z",
     "start_time": "2025-06-05T05:52:19.755006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This system prompt is a bit more complex and actually contains the function description already appended.\n",
    "# Here we suppose that the textual description of the tools have already been appended\n",
    "SYSTEM_PROMPT = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "get_weather: Get the current weather in a given location\n",
    "\n",
    "The way you use the tools is by specifying a json blob.\n",
    "Specifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\n",
    "\n",
    "The only values that should be in the \"action\" field are:\n",
    "get_weather: Get the current weather in a given location, args: {{\"location\": {{\"type\": \"string\"}}}}\n",
    "example use :\n",
    "```\n",
    "{{\n",
    "  \"action\": \"get_weather\",\n",
    "  \"action_input\": {\"location\": \"New York\"}\n",
    "}}\n",
    "\n",
    "ALWAYS use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about one action to take. Only one action at a time in this format:\n",
    "Action:\n",
    "```\n",
    "$JSON_BLOB\n",
    "```\n",
    "Observation: the result of the action. This Observation is unique, complete, and the source of truth.\n",
    "... (this Thought/Action/Observation can repeat N times, you should take several steps when needed. The $JSON_BLOB must be formatted as markdown and only use a SINGLE action at a time.)\n",
    "\n",
    "You must always end your output with the following format:\n",
    "\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Now begin! Reminder to ALWAYS use the exact characters `Final Answer:` when you provide a definitive answer. \"\"\""
   ],
   "id": "ebea2611739ddb6a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T05:55:40.696353Z",
     "start_time": "2025-06-05T05:55:40.692477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Since we are running the \"text_generation\", we need to add the right special tokens.\n",
    "prompt=f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "{SYSTEM_PROMPT}\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "What's the weather in London ?\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\""
   ],
   "id": "a52f2a56e151532a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T05:55:45.535939Z",
     "start_time": "2025-06-05T05:55:44.008844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The answer was hallucinated by the model. We need to stop to actually execute the function!\n",
    "output = client.text_generation(\n",
    "    prompt,\n",
    "    max_new_tokens=150,\n",
    "    stop=[\"Observation:\"] # Let's stop before any actual function is called\n",
    ")\n",
    "\n",
    "print(output)"
   ],
   "id": "c1af70fd6c05b1c2",
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/meta-llama/Llama-3.3-70B-Instruct (Request ID: Root=1-68413160-2ae3c5c10eb8d5df3ff122cd;89bd95a0-a279-4377-8493-d687958b723b)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\4thYear\\huggingface_course\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001B[39m, in \u001B[36mhf_raise_for_status\u001B[39m\u001B[34m(response, endpoint_name)\u001B[39m\n\u001B[32m    408\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m409\u001B[39m     \u001B[43mresponse\u001B[49m\u001B[43m.\u001B[49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    410\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\4thYear\\huggingface_course\\.venv\\Lib\\site-packages\\requests\\models.py:1024\u001B[39m, in \u001B[36mResponse.raise_for_status\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1023\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[32m-> \u001B[39m\u001B[32m1024\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response=\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[31mHTTPError\u001B[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/meta-llama/Llama-3.3-70B-Instruct",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mHfHubHTTPError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# The answer was hallucinated by the model. We need to stop to actually execute the function!\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m output = \u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtext_generation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m150\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mObservation:\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# Let's stop before any actual function is called\u001B[39;49;00m\n\u001B[32m      6\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[38;5;28mprint\u001B[39m(output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\4thYear\\huggingface_course\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2339\u001B[39m, in \u001B[36mInferenceClient.text_generation\u001B[39m\u001B[34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001B[39m\n\u001B[32m   2314\u001B[39m         _set_unsupported_text_generation_kwargs(model, unused_params)\n\u001B[32m   2315\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.text_generation(  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[32m   2316\u001B[39m             prompt=prompt,\n\u001B[32m   2317\u001B[39m             details=details,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2337\u001B[39m             watermark=watermark,\n\u001B[32m   2338\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m2339\u001B[39m     \u001B[43mraise_text_generation_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2341\u001B[39m \u001B[38;5;66;03m# Parse output\u001B[39;00m\n\u001B[32m   2342\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m stream:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\4thYear\\huggingface_course\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_common.py:410\u001B[39m, in \u001B[36mraise_text_generation_error\u001B[39m\u001B[34m(http_error)\u001B[39m\n\u001B[32m    407\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exception \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mhttp_error\u001B[39;00m\n\u001B[32m    409\u001B[39m \u001B[38;5;66;03m# Otherwise, fallback to default error\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m410\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m http_error\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\4thYear\\huggingface_course\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2309\u001B[39m, in \u001B[36mInferenceClient.text_generation\u001B[39m\u001B[34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001B[39m\n\u001B[32m   2307\u001B[39m \u001B[38;5;66;03m# Handle errors separately for more precise error messages\u001B[39;00m\n\u001B[32m   2308\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2309\u001B[39m     bytes_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_inner_post\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest_parameters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2310\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   2311\u001B[39m     match = MODEL_KWARGS_NOT_USED_REGEX.search(\u001B[38;5;28mstr\u001B[39m(e))\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\4thYear\\huggingface_course\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:280\u001B[39m, in \u001B[36mInferenceClient._inner_post\u001B[39m\u001B[34m(self, request_parameters, stream)\u001B[39m\n\u001B[32m    277\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m InferenceTimeoutError(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mInference call timed out: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrequest_parameters.url\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merror\u001B[39;00m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[32m    279\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m280\u001B[39m     \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    281\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m response.iter_lines() \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;28;01melse\u001B[39;00m response.content\n\u001B[32m    282\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m error:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\4thYear\\huggingface_course\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:482\u001B[39m, in \u001B[36mhf_raise_for_status\u001B[39m\u001B[34m(response, endpoint_name)\u001B[39m\n\u001B[32m    478\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m _format(HfHubHTTPError, message, response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m    480\u001B[39m \u001B[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001B[39;00m\n\u001B[32m    481\u001B[39m \u001B[38;5;66;03m# as well (request id and/or server error message)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m482\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m _format(HfHubHTTPError, \u001B[38;5;28mstr\u001B[39m(e), response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n",
      "\u001B[31mHfHubHTTPError\u001B[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/meta-llama/Llama-3.3-70B-Instruct (Request ID: Root=1-68413160-2ae3c5c10eb8d5df3ff122cd;89bd95a0-a279-4377-8493-d687958b723b)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Dummy function\n",
    "def get_weather(location):\n",
    "    return f\"the weather in {location} is sunny with low temperatures. \\n\"\n",
    "\n",
    "get_weather('London')"
   ],
   "id": "5082c8a5e146c492"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T05:55:56.784603Z",
     "start_time": "2025-06-05T05:55:56.756938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's concatenate the base prompt, the completion until function execution and the result of the function as an Observation\n",
    "new_prompt=prompt+output+get_weather('London')\n",
    "print(new_prompt)"
   ],
   "id": "54c6c0b50d5e08d9",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Let's concatenate the base prompt, the completion until function execution and the result of the function as an Observation\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m new_prompt=prompt+\u001B[43moutput\u001B[49m+get_weather(\u001B[33m'\u001B[39m\u001B[33mLondon\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m      3\u001B[39m \u001B[38;5;28mprint\u001B[39m(new_prompt)\n",
      "\u001B[31mNameError\u001B[39m: name 'output' is not defined"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "final_output = client.text_generation(\n",
    "    new_prompt,\n",
    "    max_new_tokens=200,\n",
    ")\n",
    "\n",
    "print(final_output)"
   ],
   "id": "da851330f1a176ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
