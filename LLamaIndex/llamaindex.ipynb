{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-11T06:10:44.698016Z",
     "start_time": "2025-06-11T06:05:19.993046Z"
    }
   },
   "source": "!pip install llama-index-llms-huggingface-api llama-index-embeddings-huggingface",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T06:19:20.993088Z",
     "start_time": "2025-06-11T06:19:14.279814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve HF_TOKEN from the environment variables\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    "    token=hf_token,\n",
    ")\n",
    "\n",
    "response = llm.complete(\"Hello, how are you?\")\n",
    "print(response)\n",
    "# I am good, how can I help you today?"
   ],
   "id": "1aa5f8c6115177a5",
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions (Request ID: Root=1-68491fe5-1aa249fc2e9664cb466b79da;3bb84ddb-b531-4e14-b226-381679494d1a)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\4thYear\\huggingface_course\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001B[39m, in \u001B[36mhf_raise_for_status\u001B[39m\u001B[34m(response, endpoint_name)\u001B[39m\n\u001B[32m    408\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m409\u001B[39m     \u001B[43mresponse\u001B[49m\u001B[43m.\u001B[49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    410\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\4thYear\\huggingface_course\\.venv\\Lib\\site-packages\\requests\\models.py:1024\u001B[39m, in \u001B[36mResponse.raise_for_status\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1023\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[32m-> \u001B[39m\u001B[32m1024\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response=\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[31mHTTPError\u001B[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mHfHubHTTPError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 18\u001B[39m\n\u001B[32m      9\u001B[39m hf_token = os.getenv(\u001B[33m\"\u001B[39m\u001B[33mHF_TOKEN\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     11\u001B[39m llm = HuggingFaceInferenceAPI(\n\u001B[32m     12\u001B[39m     model_name=\u001B[33m\"\u001B[39m\u001B[33mQwen/Qwen2.5-Coder-32B-Instruct\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     13\u001B[39m     temperature=\u001B[32m0.7\u001B[39m,\n\u001B[32m     14\u001B[39m     max_tokens=\u001B[32m100\u001B[39m,\n\u001B[32m     15\u001B[39m     token=hf_token,\n\u001B[32m     16\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m response = \u001B[43mllm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcomplete\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mHello, how are you?\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[38;5;28mprint\u001B[39m(response)\n\u001B[32m     20\u001B[39m \u001B[38;5;66;03m# I am good, how can I help you today?\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\4thYear\\huggingface_course\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:323\u001B[39m, in \u001B[36mDispatcher.span.<locals>.wrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m    320\u001B[39m             _logger.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFailed to reset active_span_id: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    322\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m323\u001B[39m     result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    324\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, asyncio.Future):\n\u001B[32m    325\u001B[39m         \u001B[38;5;66;03m# If the result is a Future, wrap it\u001B[39;00m\n\u001B[32m    326\u001B[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\4thYear\\huggingface_course\\.venv\\Lib\\site-packages\\llama_index\\llms\\huggingface_api\\base.py:314\u001B[39m, in \u001B[36mHuggingFaceInferenceAPI.complete\u001B[39m\u001B[34m(self, prompt, formatted, **kwargs)\u001B[39m\n\u001B[32m    310\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcomplete\u001B[39m(\n\u001B[32m    311\u001B[39m     \u001B[38;5;28mself\u001B[39m, prompt: \u001B[38;5;28mstr\u001B[39m, formatted: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m, **kwargs: Any\n\u001B[32m    312\u001B[39m ) -> CompletionResponse:\n\u001B[32m    313\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.task == \u001B[33m\"\u001B[39m\u001B[33mconversational\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m314\u001B[39m         chat_resp = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    315\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatMessage\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrole\u001B[49m\u001B[43m=\u001B[49m\u001B[43mMessageRole\u001B[49m\u001B[43m.\u001B[49m\u001B[43mUSER\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    316\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    317\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m chat_response_to_completion_response(chat_resp)\n\u001B[32m    319\u001B[39m     model_kwargs = \u001B[38;5;28mself\u001B[39m._get_model_kwargs(**kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\4thYear\\huggingface_course\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:323\u001B[39m, in \u001B[36mDispatcher.span.<locals>.wrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m    320\u001B[39m             _logger.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFailed to reset active_span_id: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    322\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m323\u001B[39m     result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    324\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, asyncio.Future):\n\u001B[32m    325\u001B[39m         \u001B[38;5;66;03m# If the result is a Future, wrap it\u001B[39;00m\n\u001B[32m    326\u001B[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\4thYear\\huggingface_course\\.venv\\Lib\\site-packages\\llama_index\\llms\\huggingface_api\\base.py:287\u001B[39m, in \u001B[36mHuggingFaceInferenceAPI.chat\u001B[39m\u001B[34m(self, messages, **kwargs)\u001B[39m\n\u001B[32m    284\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.task == \u001B[33m\"\u001B[39m\u001B[33mconversational\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m.task \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    285\u001B[39m     model_kwargs = \u001B[38;5;28mself\u001B[39m._get_model_kwargs(**kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m287\u001B[39m     output: ChatCompletionOutput = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sync_client\u001B[49m\u001B[43m.\u001B[49m\u001B[43mchat_completion\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    288\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_to_huggingface_messages\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    289\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    290\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    292\u001B[39m     content = output.choices[\u001B[32m0\u001B[39m].message.content \u001B[38;5;129;01mor\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    293\u001B[39m     tool_calls = output.choices[\u001B[32m0\u001B[39m].message.tool_calls \u001B[38;5;129;01mor\u001B[39;00m []\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\4thYear\\huggingface_course\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:924\u001B[39m, in \u001B[36mInferenceClient.chat_completion\u001B[39m\u001B[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001B[39m\n\u001B[32m    896\u001B[39m parameters = {\n\u001B[32m    897\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m: payload_model,\n\u001B[32m    898\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mfrequency_penalty\u001B[39m\u001B[33m\"\u001B[39m: frequency_penalty,\n\u001B[32m   (...)\u001B[39m\u001B[32m    915\u001B[39m     **(extra_body \u001B[38;5;129;01mor\u001B[39;00m {}),\n\u001B[32m    916\u001B[39m }\n\u001B[32m    917\u001B[39m request_parameters = provider_helper.prepare_request(\n\u001B[32m    918\u001B[39m     inputs=messages,\n\u001B[32m    919\u001B[39m     parameters=parameters,\n\u001B[32m   (...)\u001B[39m\u001B[32m    922\u001B[39m     api_key=\u001B[38;5;28mself\u001B[39m.token,\n\u001B[32m    923\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m924\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_inner_post\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest_parameters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    926\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[32m    927\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _stream_chat_completion_response(data)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\4thYear\\huggingface_course\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:280\u001B[39m, in \u001B[36mInferenceClient._inner_post\u001B[39m\u001B[34m(self, request_parameters, stream)\u001B[39m\n\u001B[32m    277\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m InferenceTimeoutError(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mInference call timed out: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrequest_parameters.url\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merror\u001B[39;00m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[32m    279\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m280\u001B[39m     \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    281\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m response.iter_lines() \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;28;01melse\u001B[39;00m response.content\n\u001B[32m    282\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m error:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\4thYear\\huggingface_course\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:482\u001B[39m, in \u001B[36mhf_raise_for_status\u001B[39m\u001B[34m(response, endpoint_name)\u001B[39m\n\u001B[32m    478\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m _format(HfHubHTTPError, message, response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m    480\u001B[39m \u001B[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001B[39;00m\n\u001B[32m    481\u001B[39m \u001B[38;5;66;03m# as well (request id and/or server error message)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m482\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m _format(HfHubHTTPError, \u001B[38;5;28mstr\u001B[39m(e), response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n",
      "\u001B[31mHfHubHTTPError\u001B[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions (Request ID: Root=1-68491fe5-1aa249fc2e9664cb466b79da;3bb84ddb-b531-4e14-b226-381679494d1a)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "16c285eff5471142"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
